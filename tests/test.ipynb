{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc97bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max.hammer/dev/pharia/pharia-studio-sdk/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Running Task: 100%|██████████| 1/1 [00:02<00:00,  2.00s/it]\n",
      "Evaluating: 1it [00:00, 10330.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace_id: ff52c217d07740809481e0de395a57d2\n",
      "The following error has been raised via execution {\"error\":\"Database key constraint violated\"}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "409 Client Error:  for url: http://localhost:8000/api/projects/9ab124ae-305e-42b0-99da-855e80b82c1a/evaluation/benchmarks/a46a77bd-8845-4437-a609-1c0d5b52bd28/executions/82e984e6-8bb3-44a5-9222-bfe671ea0df1/lineages",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 155\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Create and run a benchmark\u001b[39;00m\n\u001b[32m    148\u001b[39m benchmark = studio_benchmark_repository.create_benchmark(\n\u001b[32m    149\u001b[39m     dataset_id=dataset.id,\n\u001b[32m    150\u001b[39m     eval_logic=evaluation_logic,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     metadata={\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m    154\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBENCHMARK_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pharia/pharia-studio-sdk/pharia_studio_sdk/evaluation/benchmark/studio_benchmark.py:192\u001b[39m, in \u001b[36mStudioBenchmark.execute\u001b[39m\u001b[34m(self, task, name, description, labels, metadata, max_workers)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrace_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrace_ids[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    186\u001b[39m benchmark_lineages = \u001b[38;5;28mself\u001b[39m._create_benchmark_lineages(\n\u001b[32m    187\u001b[39m     eval_lineages=evaluation_lineages,\n\u001b[32m    188\u001b[39m     trace_ids=trace_ids,\n\u001b[32m    189\u001b[39m     latencies_per_trace=latency_per_trace,\n\u001b[32m    190\u001b[39m     tokens_per_trace=tokens_per_trace,\n\u001b[32m    191\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit_benchmark_lineages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbenchmark_lineages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbenchmark_lineages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbenchmark_execution_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbenchmark_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m benchmark_execution_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pharia/pharia-studio-sdk/pharia_studio_sdk/connectors/studio/studio.py:575\u001b[39m, in \u001b[36mStudioClient.submit_benchmark_lineages\u001b[39m\u001b[34m(self, benchmark_lineages, benchmark_id, execution_id, max_payload_size)\u001b[39m\n\u001b[32m    571\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch:\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# Send batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_compressed_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_id\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m     all_responses.extend(response)\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Only reached if a lineage is too big for the request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pharia/pharia-studio-sdk/pharia_studio_sdk/connectors/studio/studio.py:624\u001b[39m, in \u001b[36mStudioClient._send_compressed_batch\u001b[39m\u001b[34m(self, batch, benchmark_id, execution_id)\u001b[39m\n\u001b[32m    616\u001b[39m headers = {**\u001b[38;5;28mself\u001b[39m._headers, \u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    618\u001b[39m response = requests.post(\n\u001b[32m    619\u001b[39m     url,\n\u001b[32m    620\u001b[39m     headers=headers,\n\u001b[32m    621\u001b[39m     data=compressed_data,\n\u001b[32m    622\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pharia/pharia-studio-sdk/pharia_studio_sdk/connectors/studio/studio.py:634\u001b[39m, in \u001b[36mStudioClient._raise_for_status\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    632\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following error has been raised via execution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.response.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    633\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pharia/pharia-studio-sdk/pharia_studio_sdk/connectors/studio/studio.py:629\u001b[39m, in \u001b[36mStudioClient._raise_for_status\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_raise_for_status\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: requests.Response) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    631\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    632\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following error has been raised via execution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.response.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    633\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/pharia/pharia-studio-sdk/.venv/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 409 Client Error:  for url: http://localhost:8000/api/projects/9ab124ae-305e-42b0-99da-855e80b82c1a/evaluation/benchmarks/a46a77bd-8845-4437-a609-1c0d5b52bd28/executions/82e984e6-8bb3-44a5-9222-bfe671ea0df1/lineages"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections.abc import Iterable\n",
    "from statistics import mean\n",
    "\n",
    "from aleph_alpha_client import Client\n",
    "from pharia_inference_sdk.core import (\n",
    "    CompleteInput,\n",
    "    ControlModel,\n",
    "    Llama3InstructModel,\n",
    "    Task,\n",
    "    TaskSpan,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from pharia_studio_sdk import StudioClient\n",
    "from pharia_studio_sdk.evaluation import (\n",
    "    AggregationLogic,\n",
    "    Example,\n",
    "    SingleOutputEvaluationLogic,\n",
    "    StudioBenchmarkRepository,\n",
    "    StudioDatasetRepository,\n",
    ")\n",
    "\n",
    "# Studio Configuration\n",
    "PROJECT_NAME = \"My first project\"\n",
    "BENCHMARK_NAME = \"My first benchmark\"\n",
    "STUDIO_URL = os.getenv(\"STUDIO_URL\")\n",
    "\n",
    "# Inference Configuration\n",
    "INFERENCE_URL = os.getenv(\"CLIENT_URL\")\n",
    "AA_TOKEN = os.getenv(\"AA_TOKEN\")\n",
    "\n",
    "# Define a task with the `pharia-inference-sdk` module\n",
    "class ExtractIngredientsTaskInput(BaseModel):\n",
    "    recipe: str\n",
    "\n",
    "class ExtractIngredientsTaskOutput(BaseModel):\n",
    "    ingredients: list[str]\n",
    "\n",
    "class ExtractIngredientsTask(Task[ExtractIngredientsTaskInput, ExtractIngredientsTaskOutput]):\n",
    "    PROMPT_TEMPLATE = \"\"\"Given the following recipe, extract the list of ingredients. Write one ingredient per line, do not add the quantity, do not add any text besides the list.\n",
    "\n",
    "Recipe:\n",
    "{recipe}\n",
    "\"\"\"\n",
    "    def __init__(self, model: ControlModel):\n",
    "        self._model = model\n",
    "\n",
    "\n",
    "    def do_run(\n",
    "            self, input: ExtractIngredientsTaskInput, task_span: TaskSpan\n",
    "        ) -> ExtractIngredientsTaskOutput:\n",
    "\n",
    "            prompt = self._model.to_instruct_prompt(\n",
    "                self.PROMPT_TEMPLATE.format(recipe=input.recipe)\n",
    "            )\n",
    "\n",
    "            completion_input = CompleteInput(\n",
    "                 prompt=prompt,\n",
    "                 model=self._model)\n",
    "            completion = self._model.complete(completion_input, tracer=task_span)\n",
    "            ingredients = completion.completions[0].completion.split('\\n')\n",
    "            return ExtractIngredientsTaskOutput(ingredients=ingredients)\n",
    "\n",
    "# Define a logic for evaluation\n",
    "class IngredientsEvaluation(BaseModel):\n",
    "    correct_number_of_ingredients: bool\n",
    "\n",
    "class IngredientsAggregatedEvaluation(BaseModel):\n",
    "    avg_result: float\n",
    "\n",
    "class IngredientsEvaluationLogic(\n",
    "    SingleOutputEvaluationLogic[\n",
    "        ExtractIngredientsTaskInput,\n",
    "        ExtractIngredientsTaskOutput,\n",
    "        ExtractIngredientsTaskOutput,\n",
    "        IngredientsEvaluation,\n",
    "    ]\n",
    "):\n",
    "    def do_evaluate_single_output(\n",
    "        self,\n",
    "        example: Example[ExtractIngredientsTaskInput, ExtractIngredientsTaskOutput],\n",
    "        output: ExtractIngredientsTaskOutput,\n",
    "    ) -> IngredientsEvaluation:\n",
    "        return IngredientsEvaluation(\n",
    "            correct_number_of_ingredients=len(output.ingredients) == len(example.expected_output.ingredients),\n",
    "        )\n",
    "class IngredientsAggregationLogic(\n",
    "    AggregationLogic[IngredientsEvaluation, IngredientsAggregatedEvaluation]\n",
    "):\n",
    "    def aggregate(\n",
    "        self, evaluations: Iterable[IngredientsEvaluation]\n",
    "    ) -> IngredientsAggregatedEvaluation:\n",
    "        evaluation_list = list(evaluations)\n",
    "        return IngredientsAggregatedEvaluation(\n",
    "            avg_result=mean(evaluation.correct_number_of_ingredients for evaluation in evaluation_list)\n",
    "        )\n",
    "\n",
    "aa_client = Client(token=AA_TOKEN, host=INFERENCE_URL)\n",
    "studio_client = StudioClient(PROJECT_NAME,studio_url=STUDIO_URL, auth_token=AA_TOKEN, create_project=True)\n",
    "studio_benchmark_repository = StudioBenchmarkRepository(studio_client=studio_client)\n",
    "studio_dataset_repository = StudioDatasetRepository(studio_client=studio_client)\n",
    "\n",
    "evaluation_logic = IngredientsEvaluationLogic()\n",
    "aggregation_logic = IngredientsAggregationLogic()\n",
    "\n",
    "# Create a dataset with example inputs and expected outputs\n",
    "examples = [   Example(\n",
    "        input=ExtractIngredientsTaskInput(\n",
    "            recipe=\"\"\"# Pike Burger\n",
    "\n",
    "- Pike (the bigger the better)\n",
    "- Breadcrumbs\n",
    "- Onions\n",
    "- Garlic\n",
    "- Eggs\n",
    "- Mustard\n",
    "- Flour\n",
    "- Spices\n",
    "- Salt\n",
    "- Pepper\n",
    "- Oil\n",
    "\n",
    "1. Fish pike\n",
    "2. Filet fish into pieces\n",
    "3. Grind the fish and onions into a paste\n",
    "4. Mix paste with all other ingredients beside the flour and oil, add breadcrumbs until the consistency is right for forming patties\n",
    "5. Form the paste into patties and coat them in flour\n",
    "6. Let them rest in the fridge for 30min to firm up\n",
    "7. Shallow fry them in a pan with a lot of oil\n",
    "\"\"\"\n",
    "        ),\n",
    "        expected_output=ExtractIngredientsTaskOutput(\n",
    "            ingredients=[\n",
    "               \"Pike\", \"Breadcrumbs\", \"Onions\", \"Garlic\", \"Eggs\", \"Mustard\", \"Flour\", \"Spices\", \"Salt\", \"Pepper\", \"Oil\"\n",
    "            ]\n",
    "        ),\n",
    "    )]\n",
    "dataset = studio_dataset_repository.create_dataset(\n",
    "    examples=examples,\n",
    "    dataset_name=\"My first dataset\",\n",
    "    metadata={\"description\": \"dataset_description\"},\n",
    ")\n",
    "\n",
    "model = Llama3InstructModel(name=\"llama-3.1-8b-instruct\", client=aa_client)\n",
    "task = ExtractIngredientsTask(model=model)\n",
    "# Create and run a benchmark\n",
    "benchmark = studio_benchmark_repository.create_benchmark(\n",
    "    dataset_id=dataset.id,\n",
    "    eval_logic=evaluation_logic,\n",
    "    aggregation_logic=aggregation_logic,\n",
    "    name=BENCHMARK_NAME,\n",
    "    metadata={\"key\": \"value\"},\n",
    ")\n",
    "benchmark.execute(\n",
    "    task=task,\n",
    "    name=BENCHMARK_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4025de1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#37d52e84-3c33-4772-a1ea-75ff8a26b671"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
